{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c939b650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:48:58.713290Z",
     "iopub.status.busy": "2025-05-24T02:48:58.713014Z",
     "iopub.status.idle": "2025-05-24T02:48:58.716918Z",
     "shell.execute_reply": "2025-05-24T02:48:58.716248Z"
    },
    "papermill": {
     "duration": 0.009674,
     "end_time": "2025-05-24T02:48:58.718071",
     "exception": false,
     "start_time": "2025-05-24T02:48:58.708397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf0ff1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:48:58.724926Z",
     "iopub.status.busy": "2025-05-24T02:48:58.724401Z",
     "iopub.status.idle": "2025-05-24T02:49:02.687258Z",
     "shell.execute_reply": "2025-05-24T02:49:02.686573Z"
    },
    "papermill": {
     "duration": 3.967445,
     "end_time": "2025-05-24T02:49:02.688596",
     "exception": false,
     "start_time": "2025-05-24T02:48:58.721151",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca3580b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:49:02.695787Z",
     "iopub.status.busy": "2025-05-24T02:49:02.695470Z",
     "iopub.status.idle": "2025-05-24T02:49:12.223877Z",
     "shell.execute_reply": "2025-05-24T02:49:12.223268Z"
    },
    "papermill": {
     "duration": 9.533346,
     "end_time": "2025-05-24T02:49:12.225220",
     "exception": false,
     "start_time": "2025-05-24T02:49:02.691874",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Chuyển thành chữ thường\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Loại bỏ tất cả dấu câu ngoại trừ dấu chấm\n",
    "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "    \n",
    "    # Thay thế nhiều khoảng trắng bằng một khoảng trắng duy nhất và loại bỏ khoảng trắng ở đầu/cuối\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Dataset paths\n",
    "dataset_path = '/kaggle/input/en-vi-data/'\n",
    "en_file_path = dataset_path + 'TED2020.en-vi.en'\n",
    "vi_file_path = dataset_path + 'TED2020.en-vi.vi'\n",
    "\n",
    "# Read and clean data\n",
    "try:\n",
    "    with open(en_file_path, 'r', encoding='utf-8') as en_file, \\\n",
    "         open(vi_file_path, 'r', encoding='utf-8') as vi_file, \\\n",
    "         open('cleaned.en', 'w', encoding='utf-8') as en_out, \\\n",
    "         open('cleaned.vi', 'w', encoding='utf-8') as vi_out:\n",
    "        for en_line, vi_line in zip(en_file, vi_file):\n",
    "            en_clean = clean_text(en_line)\n",
    "            vi_clean = clean_text(vi_line)\n",
    "            if en_clean and vi_clean and len(en_clean.split()) > 1 and len(vi_clean.split()) > 1:\n",
    "                en_out.write(en_clean + '\\n')\n",
    "                vi_out.write(vi_clean + '\\n')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: File not found. Check the file paths: {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data cleaning: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Combine data for GPT (English -> Vietnamese)\n",
    "try:\n",
    "    with open('cleaned.en', 'r', encoding='utf-8') as en_file, \\\n",
    "         open('cleaned.vi', 'r', encoding='utf-8') as vi_file, \\\n",
    "         open('gpt_data.txt', 'w', encoding='utf-8') as out_file:\n",
    "        for en_line, vi_line in zip(en_file, vi_file):\n",
    "            combined = f\"Source: {en_line.strip()} Target: {vi_line.strip()} [EOS]\\n\"\n",
    "            out_file.write(combined)\n",
    "except Exception as e:\n",
    "    print(f\"Error combining data: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85b093c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:49:12.231954Z",
     "iopub.status.busy": "2025-05-24T02:49:12.231726Z",
     "iopub.status.idle": "2025-05-24T02:49:12.235103Z",
     "shell.execute_reply": "2025-05-24T02:49:12.234581Z"
    },
    "papermill": {
     "duration": 0.007785,
     "end_time": "2025-05-24T02:49:12.236177",
     "exception": false,
     "start_time": "2025-05-24T02:49:12.228392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# # Train SentencePiece with BPE\n",
    "# \"\"\"\n",
    "# BPE (Byte Pair Encoding) là một thuật toán nén và token hóa văn bản. \n",
    "# Cách hoạt động:\n",
    "# 1. Bắt đầu với tất cả các ký tự riêng lẻ trong văn bản (bao gồm cả ký hiệu đặc biệt).\n",
    "# 2. Lặp lại: Tìm cặp ký tự (hoặc subword) xuất hiện thường xuyên nhất và hợp nhất chúng thành một token mới.\n",
    "# 3. Tiếp tục hợp nhất cho đến khi đạt được kích thước từ vựng mong muốn (vocab_size).\n",
    "# 4. Kết quả là một từ vựng chứa các subword, giúp xử lý các từ hiếm hoặc từ mới hiệu quả.\n",
    "# SentencePiece áp dụng BPE để tạo ra các token subword, phù hợp với các tác vụ dịch máy.\n",
    "# \"\"\"\n",
    "# start_time = time.time()\n",
    "# spm.SentencePieceTrainer.train(\n",
    "#     input='gpt_data.txt',\n",
    "#     model_prefix='ted2020_spm',\n",
    "#     vocab_size=52000,\n",
    "#     character_coverage=1.0,\n",
    "#     model_type='bpe',\n",
    "#     user_defined_symbols=['[EOS]', '[PAD]', '[BOS]', 'Source:', 'Target:'],\n",
    "#     pad_id=0,\n",
    "#     bos_id=1,\n",
    "#     eos_id=2,\n",
    "#     unk_id=3,\n",
    "#     num_threads=2\n",
    "# )\n",
    "# print(f\"Training spm took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Kết quả train trong máy\n",
    "# Training spm took 20.18 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70e58422",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:49:12.242617Z",
     "iopub.status.busy": "2025-05-24T02:49:12.242419Z",
     "iopub.status.idle": "2025-05-24T02:49:50.603994Z",
     "shell.execute_reply": "2025-05-24T02:49:50.603094Z"
    },
    "papermill": {
     "duration": 38.366251,
     "end_time": "2025-05-24T02:49:50.605204",
     "exception": false,
     "start_time": "2025-05-24T02:49:12.238953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files copied to /kaggle/working: ['ted2020_spm.vocab', '__notebook__.ipynb', 'cleaned.en', 'ted2020_spm.model', 'gpt_data.txt', 'cleaned.vi']\n",
      "Loading tokenizer took 0.04 seconds\n",
      "Sample tokens from BPE vocabulary:\n",
      "Token 0: <pad>\n",
      "Token 1: <s>\n",
      "Token 2: </s>\n",
      "Token 3: <unk>\n",
      "Token 4: [EOS]\n",
      "Token 5: [PAD]\n",
      "Token 6: [BOS]\n",
      "Token 7: Source:\n",
      "Token 8: Target:\n",
      "Token 9: ▁t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data with BPE: 100%|██████████| 320309/320309 [00:37<00:00, 8466.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data took 38.29 seconds\n",
      "Pad ID: 0\n",
      "BOS ID: 1\n",
      "EOS ID: 2\n",
      "UNK ID: 3\n",
      "Vocab size: 52000\n",
      "Example sentence: Source: i like to learn new things Target: tôi thích học những thứ mới [EOS]\n",
      "BPE tokenized: ['▁', 'Source:', '▁i', '▁like', '▁to', '▁learn', '▁new', '▁things', '▁', 'Target:', '▁tôi', '▁thích', '▁học', '▁những', '▁thứ', '▁mới', '▁', '[EOS]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# Kiểm tra và sao chép file từ input sang working directory\n",
    "model_path = '/kaggle/input/ted2020-spm/ted2020_spm.model'\n",
    "vocab_path = '/kaggle/input/ted2020-spm/ted2020_spm.vocab'\n",
    "working_model_path = '/kaggle/working/ted2020_spm.model'\n",
    "working_vocab_path = '/kaggle/working/ted2020_spm.vocab'\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.exists(vocab_path):\n",
    "    print(\"Error: Model or vocab file not found in /kaggle/input/ted2020-spm-files\")\n",
    "    exit(1)\n",
    "\n",
    "shutil.copy(model_path, working_model_path)\n",
    "shutil.copy(vocab_path, working_vocab_path)\n",
    "print(\"Files copied to /kaggle/working:\", os.listdir('/kaggle/working/'))\n",
    "\n",
    "# Tải tokenizer từ file đã tải lên\n",
    "start_time = time.time()\n",
    "sp = spm.SentencePieceProcessor(model_file='ted2020_spm.model')\n",
    "print(f\"Loading tokenizer took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Hiển thị một số token trong từ vựng để minh họa BPE\n",
    "print(\"Sample tokens from BPE vocabulary:\")\n",
    "with open('ted2020_spm.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    for i, line in enumerate(vocab_file):\n",
    "        if i >= 10:  # In 10 token đầu tiên\n",
    "            break\n",
    "        token, _ = line.strip().split('\\t')\n",
    "        print(f\"Token {i}: {token}\")\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_gpt_file(input_file, output_file, sp):\n",
    "    \"\"\"\n",
    "    Hàm này mã hóa văn bản thành các token số nguyên sử dụng BPE.\n",
    "    Mỗi dòng văn bản được chia thành các subword dựa trên từ vựng BPE đã huấn luyện.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "            lines = f_in.readlines()\n",
    "        with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "            for line in tqdm(lines, desc=\"Tokenizing data with BPE\"):\n",
    "                tokens = sp.encode(line.strip(), out_type=int)\n",
    "                valid_tokens = [t for t in tokens if 0 <= t < 52000]  # Giới hạn khớp với vocab_size đã train\n",
    "                if valid_tokens:\n",
    "                    f_out.write(' '.join(map(str, valid_tokens)) + '\\n')\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing data: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "# Kiểm tra file gpt_data.txt\n",
    "if not os.path.exists('gpt_data.txt'):\n",
    "    print(\"Error: gpt_data.txt not found. Please run the data preparation steps first.\")\n",
    "    exit(1)\n",
    "\n",
    "start_time = time.time()\n",
    "tokenize_gpt_file('gpt_data.txt', 'tokenized_gpt.txt', sp)\n",
    "print(f\"Tokenizing data took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Verify special tokens\n",
    "print(f\"Pad ID: {sp.pad_id()}\")\n",
    "print(f\"BOS ID: {sp.bos_id()}\")\n",
    "print(f\"EOS ID: {sp.eos_id()}\")\n",
    "print(f\"UNK ID: {sp.unk_id()}\")\n",
    "print(f\"Vocab size: {sp.get_piece_size()}\")\n",
    "\n",
    "# Minh họa cách BPE mã hóa một câu ví dụ\n",
    "example_sentence = \"Source: i like to learn new things Target: tôi thích học những thứ mới [EOS]\"\n",
    "encoded = sp.encode(example_sentence, out_type=str)  # out_type=str để thấy các subword\n",
    "print(f\"Example sentence: {example_sentence}\")\n",
    "print(f\"BPE tokenized: {encoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e80e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:49:50.639378Z",
     "iopub.status.busy": "2025-05-24T02:49:50.639117Z",
     "iopub.status.idle": "2025-05-24T02:49:55.354016Z",
     "shell.execute_reply": "2025-05-24T02:49:55.353307Z"
    },
    "papermill": {
     "duration": 4.73306,
     "end_time": "2025-05-24T02:49:55.355132",
     "exception": false,
     "start_time": "2025-05-24T02:49:50.622072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving train data: 100%|██████████| 256247/256247 [00:00<00:00, 1515605.24it/s]\n",
      "Saving val data: 100%|██████████| 32031/32031 [00:00<00:00, 1570014.97it/s]\n",
      "Saving test data: 100%|██████████| 32031/32031 [00:00<00:00, 1547874.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    with open('/kaggle/working/tokenized_gpt.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Remove empty or invalid token lines\n",
    "    lines = [line for line in lines if line.strip() and all(0 <= int(t) < 52000 for t in line.strip().split())]\n",
    "\n",
    "    # Split data\n",
    "    train_lines, temp_lines = train_test_split(lines, test_size=0.2, random_state=42)\n",
    "    val_lines, test_lines = train_test_split(temp_lines, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Save splits\n",
    "    for split, data in [('train', train_lines), ('val', val_lines), ('test', test_lines)]:\n",
    "        with open(f'{split}_gpt.txt', 'w', encoding='utf-8') as f:\n",
    "            for line in tqdm(data, desc=f\"Saving {split} data\"):\n",
    "                f.write(line)\n",
    "except Exception as e:\n",
    "    print(f\"Error splitting data: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c6ae14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:49:55.390125Z",
     "iopub.status.busy": "2025-05-24T02:49:55.389778Z",
     "iopub.status.idle": "2025-05-24T02:49:55.400079Z",
     "shell.execute_reply": "2025-05-24T02:49:55.399538Z"
    },
    "papermill": {
     "duration": 0.028941,
     "end_time": "2025-05-24T02:49:55.401145",
     "exception": false,
     "start_time": "2025-05-24T02:49:55.372204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attn_output, _ = self.self_attn(x, x, x, attn_mask=attn_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        return x\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4, dim_feedforward=1024, max_seq_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = self.create_positional_encoding(max_seq_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            DecoderBlock(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_positional_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n",
    "\n",
    "    def generate_mask(self, sz):\n",
    "        return torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.size()\n",
    "        device = x.device\n",
    "        emb = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        pe = self.positional_encoding[:T].to(device)\n",
    "        x = self.dropout(emb + pe)\n",
    "\n",
    "        mask = self.generate_mask(T).to(device)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask=mask)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return self.fc_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fb4346b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:49:55.435261Z",
     "iopub.status.busy": "2025-05-24T02:49:55.435028Z",
     "iopub.status.idle": "2025-05-24T02:49:55.591137Z",
     "shell.execute_reply": "2025-05-24T02:49:55.590549Z"
    },
    "papermill": {
     "duration": 0.174635,
     "end_time": "2025-05-24T02:49:55.592601",
     "exception": false,
     "start_time": "2025-05-24T02:49:55.417966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, file_path, sp, max_len=512, vocab_size=52000):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            self.data = [line.strip() for line in f if line.strip()]\n",
    "        self.sp = sp\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = list(map(int, self.data[idx].split()))\n",
    "        tokens = [t for t in tokens if 0 <= t < self.vocab_size]\n",
    "        if len(tokens) > self.max_len:\n",
    "            tokens = tokens[:self.max_len]\n",
    "        if not tokens:\n",
    "            tokens = [self.sp.eos_id()]\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return pad_sequence(batch, batch_first=True, padding_value=sp.pad_id())\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = GPTDataset('train_gpt.txt', sp)\n",
    "val_dataset = GPTDataset('val_gpt.txt', sp)\n",
    "test_dataset = GPTDataset('test_gpt.txt', sp)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242db9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T02:49:55.627361Z",
     "iopub.status.busy": "2025-05-24T02:49:55.627134Z",
     "iopub.status.idle": "2025-05-24T09:12:02.237368Z",
     "shell.execute_reply": "2025-05-24T09:12:02.236526Z"
    },
    "papermill": {
     "duration": 22926.629262,
     "end_time": "2025-05-24T09:12:02.238999",
     "exception": false,
     "start_time": "2025-05-24T02:49:55.609737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 16016/16016 [12:14<00:00, 21.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4.813722338962864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 1: 100%|██████████| 2002/2002 [00:31<00:00, 64.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 4.328433278795484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 16016/16016 [12:13<00:00, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 4.23287068905828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 2: 100%|██████████| 2002/2002 [00:31<00:00, 64.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Val Loss: 4.029951311729767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 16016/16016 [12:11<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 4.007410544853706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 3: 100%|██████████| 2002/2002 [00:30<00:00, 64.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val Loss: 3.8580496536268223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 16016/16016 [12:10<00:00, 21.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 3.8655814961953596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 4: 100%|██████████| 2002/2002 [00:31<00:00, 64.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Val Loss: 3.748926467590637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 16016/16016 [12:12<00:00, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 3.767647618001753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 5: 100%|██████████| 2002/2002 [00:30<00:00, 64.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Val Loss: 3.6831810588722345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 16016/16016 [12:13<00:00, 21.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 3.696829429843447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 6: 100%|██████████| 2002/2002 [00:30<00:00, 64.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Val Loss: 3.635322645708517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 16016/16016 [12:10<00:00, 21.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 3.6450210987003175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 7: 100%|██████████| 2002/2002 [00:30<00:00, 64.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Val Loss: 3.5993533620348463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 16016/16016 [12:11<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 3.608138964294673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 8: 100%|██████████| 2002/2002 [00:31<00:00, 64.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Val Loss: 3.580244950004867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 16016/16016 [12:11<00:00, 21.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 3.5842207585360026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 9: 100%|██████████| 2002/2002 [00:30<00:00, 64.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Val Loss: 3.566252657107183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 16016/16016 [12:11<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 3.5707397282748787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 10: 100%|██████████| 2002/2002 [00:31<00:00, 64.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Val Loss: 3.5623850232952243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 11: 100%|██████████| 16016/16016 [12:12<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train Loss: 3.566195347955772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 11: 100%|██████████| 2002/2002 [00:30<00:00, 64.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Val Loss: 3.5623850232952243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 12: 100%|██████████| 16016/16016 [12:12<00:00, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train Loss: 3.567967985015173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 12: 100%|██████████| 2002/2002 [00:30<00:00, 64.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Val Loss: 3.560816026472307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 13: 100%|██████████| 16016/16016 [12:11<00:00, 21.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train Loss: 3.5717953396188866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 13: 100%|██████████| 2002/2002 [00:30<00:00, 64.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Val Loss: 3.557174640578347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 14: 100%|██████████| 16016/16016 [12:11<00:00, 21.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train Loss: 3.572891145810619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 14: 100%|██████████| 2002/2002 [00:30<00:00, 64.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Val Loss: 3.552530506274083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 15: 100%|██████████| 16016/16016 [12:10<00:00, 21.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train Loss: 3.5709669942503326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 15: 100%|██████████| 2002/2002 [00:31<00:00, 64.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Val Loss: 3.53826524958863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 16: 100%|██████████| 16016/16016 [12:12<00:00, 21.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train Loss: 3.5638708568119504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 16: 100%|██████████| 2002/2002 [00:31<00:00, 64.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Val Loss: 3.5253925365167897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 17: 100%|██████████| 16016/16016 [12:11<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train Loss: 3.5513978373784045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 17: 100%|██████████| 2002/2002 [00:31<00:00, 64.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Val Loss: 3.517922284005286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 18: 100%|██████████| 16016/16016 [12:12<00:00, 21.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train Loss: 3.5343790216581685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 18: 100%|██████████| 2002/2002 [00:30<00:00, 64.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Val Loss: 3.4979139010270277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 19: 100%|██████████| 16016/16016 [12:12<00:00, 21.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train Loss: 3.513723875050778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 19: 100%|██████████| 2002/2002 [00:31<00:00, 64.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Val Loss: 3.487673608811347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 20: 100%|██████████| 16016/16016 [12:12<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train Loss: 3.490398268272112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 20: 100%|██████████| 2002/2002 [00:31<00:00, 64.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Val Loss: 3.4629324716287893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 21: 100%|██████████| 16016/16016 [12:13<00:00, 21.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Train Loss: 3.4650338852887863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 21: 100%|██████████| 2002/2002 [00:31<00:00, 64.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Val Loss: 3.4461595252320008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 22: 100%|██████████| 16016/16016 [12:13<00:00, 21.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Train Loss: 3.437164400706996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 22: 100%|██████████| 2002/2002 [00:31<00:00, 64.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Val Loss: 3.434322984306724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 23: 100%|██████████| 16016/16016 [12:16<00:00, 21.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Train Loss: 3.407415288088324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 23: 100%|██████████| 2002/2002 [00:31<00:00, 64.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Val Loss: 3.410097435280517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 24: 100%|██████████| 16016/16016 [12:17<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Train Loss: 3.3775672141935202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 24: 100%|██████████| 2002/2002 [00:31<00:00, 64.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Val Loss: 3.388098214294289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 25: 100%|██████████| 16016/16016 [12:12<00:00, 21.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Train Loss: 3.3468494596300307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 25: 100%|██████████| 2002/2002 [00:31<00:00, 63.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Val Loss: 3.3741534050408895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 26: 100%|██████████| 16016/16016 [12:12<00:00, 21.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Train Loss: 3.3181048320396083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 26: 100%|██████████| 2002/2002 [00:31<00:00, 64.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Val Loss: 3.358704218497643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 27: 100%|██████████| 16016/16016 [12:13<00:00, 21.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Train Loss: 3.2913252426223916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 27: 100%|██████████| 2002/2002 [00:31<00:00, 64.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Val Loss: 3.344863273523428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 28: 100%|██████████| 16016/16016 [12:14<00:00, 21.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Train Loss: 3.268971726700262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 28: 100%|██████████| 2002/2002 [00:30<00:00, 64.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Val Loss: 3.3363433579703075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 29: 100%|██████████| 16016/16016 [12:11<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Train Loss: 3.251268305546873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 29: 100%|██████████| 2002/2002 [00:31<00:00, 64.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Val Loss: 3.329618592838665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 30: 100%|██████████| 16016/16016 [12:11<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Train Loss: 3.2401712729737953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Epoch 30: 100%|██████████| 2002/2002 [00:31<00:00, 64.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Val Loss: 3.327551039067896\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPTModel(vocab_size=52000).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=sp.pad_id())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        batch = batch.to(device)\n",
    "        if torch.any(batch < 0) or torch.any(batch >= 52000):\n",
    "            print(\"Warning: Invalid token detected in batch\")\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[:, :-1]\n",
    "        target_ids = batch[:, 1:]\n",
    "        output = model(input_ids)\n",
    "        loss = criterion(output.reshape(-1, 52000), target_ids.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}')\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validating Epoch {epoch+1}\"):\n",
    "            batch = batch.to(device)\n",
    "            if torch.any(batch < 0) or torch.any(batch >= 52000):\n",
    "                print(\"Warning: Invalid token detected in validation batch\")\n",
    "                continue\n",
    "            input_ids = batch[:, :-1]\n",
    "            target_ids = batch[:, 1:]\n",
    "            output = model(input_ids)\n",
    "            loss = criterion(output.reshape(-1, 52000), target_ids.reshape(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f'Epoch {epoch+1}, Val Loss: {val_loss}')\n",
    "    scheduler.step()\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56570743",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T09:12:16.660722Z",
     "iopub.status.busy": "2025-05-24T09:12:16.660017Z",
     "iopub.status.idle": "2025-05-24T09:12:16.667003Z",
     "shell.execute_reply": "2025-05-24T09:12:16.666361Z"
    },
    "papermill": {
     "duration": 7.255254,
     "end_time": "2025-05-24T09:12:16.668158",
     "exception": false,
     "start_time": "2025-05-24T09:12:09.412904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from nltk.translate.bleu_score import corpus_bleu\n",
    "# from nltk.translate.meteor_score import meteor_score\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# def extract_target(text, sp):\n",
    "#     try:\n",
    "#         decoded = sp.decode(text)\n",
    "#         if 'Target:' in decoded:\n",
    "#             target = decoded.split('Target:')[1].strip()\n",
    "#             if '[EOS]' in target:\n",
    "#                 target = target.split('[EOS]')[0].strip()\n",
    "#             return target\n",
    "#         print(f\"Warning: No 'Target:' in decoded text: {decoded}\")\n",
    "#         return ''\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error decoding text: {e}, text: {text}\")\n",
    "#         return ''\n",
    "\n",
    "# model.eval()\n",
    "# references = []\n",
    "# hypotheses = []\n",
    "# meteor_scores = []\n",
    "# max_gen_len = 200\n",
    "# max_samples = 500\n",
    "# max_seq_len = 512  # Giới hạn độ dài chuỗi, khớp với max_seq_len của mô hình\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\", total=min(max_samples, len(test_loader)))):\n",
    "#         if batch_idx >= max_samples:\n",
    "#             break\n",
    "\n",
    "#         batch = batch.to(device)\n",
    "#         if torch.any(batch < 0) or torch.any(batch >= 32000):\n",
    "#             print(f\"Warning: Invalid token in batch {batch_idx}\")\n",
    "#             continue\n",
    "\n",
    "#         input_ids = batch[:, :-1]\n",
    "#         generated = input_ids[0].tolist()\n",
    "\n",
    "#         # Debug: In token đầu vào\n",
    "#         # if batch_idx < 5:\n",
    "#         #     print(f\"Batch {batch_idx} input tokens: {generated}\")\n",
    "#         #     print(f\"Decoded input: {sp.decode(generated)}\")\n",
    "\n",
    "#         for _ in range(max_gen_len):\n",
    "#             # Cắt chuỗi nếu vượt quá max_seq_len\n",
    "#             if len(generated) > max_seq_len:\n",
    "#                 generated = generated[:max_seq_len]\n",
    "#                 print(f\"Warning: Truncated sequence to {max_seq_len} tokens in batch {batch_idx}\")\n",
    "#                 break\n",
    "\n",
    "#             input_tensor = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "#             output = model(input_tensor)\n",
    "#             next_token = output[:, -1, :].argmax(dim=-1).item()\n",
    "#             if next_token < 0 or next_token >= 32000:\n",
    "#                 print(f\"Warning: Invalid token generated in batch {batch_idx}: {next_token}\")\n",
    "#                 break\n",
    "#             generated.append(next_token)\n",
    "#             if next_token == sp.eos_id():\n",
    "#                 break\n",
    "\n",
    "#         # Debug: In token sinh ra\n",
    "#         # if batch_idx < 5:\n",
    "#         #     print(f\"Batch {batch_idx} generated tokens: {generated}\")\n",
    "#         #     print(f\"Decoded generated: {sp.decode(generated)}\")\n",
    "\n",
    "#         hyp = extract_target(generated, sp)\n",
    "#         ref = extract_target(batch[0].tolist(), sp)\n",
    "\n",
    "#         if not hyp or not ref:\n",
    "#             print(f\"Batch {batch_idx}: Empty hyp or ref: hyp='{hyp}', ref='{ref}'\")\n",
    "#             continue\n",
    "\n",
    "#         # Tokenize hyp và ref thành danh sách các từ\n",
    "#         hyp_tokens = hyp.split()\n",
    "#         ref_tokens = ref.split()\n",
    "\n",
    "#         # Lưu cho BLEU score\n",
    "#         references.append([ref_tokens])\n",
    "#         hypotheses.append(hyp_tokens)\n",
    "\n",
    "#         # Tính METEOR score\n",
    "#         try:\n",
    "#             score = meteor_score([ref_tokens], hyp_tokens)\n",
    "#             meteor_scores.append(score)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error calculating METEOR score for batch {batch_idx}: {e}\")\n",
    "#             continue\n",
    "\n",
    "# # Tính BLEU và METEOR\n",
    "# if references and hypotheses:\n",
    "#     bleu = corpus_bleu(references, hypotheses)\n",
    "#     avg_meteor = sum(meteor_scores) / len(meteor_scores) if meteor_scores else 0\n",
    "#     print(f'BLEU Score: {bleu:.4f}')\n",
    "#     print(f'Average METEOR Score: {avg_meteor:.4f}')\n",
    "#     print(f'Number of valid samples: {len(references)}')\n",
    "# else:\n",
    "#     print(\"Error: No valid references or hypotheses for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d79bcf6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T09:12:31.223654Z",
     "iopub.status.busy": "2025-05-24T09:12:31.223111Z",
     "iopub.status.idle": "2025-05-24T09:12:36.941163Z",
     "shell.execute_reply": "2025-05-24T09:12:36.940132Z"
    },
    "papermill": {
     "duration": 12.996205,
     "end_time": "2025-05-24T09:12:36.942972",
     "exception": false,
     "start_time": "2025-05-24T09:12:23.946767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\r\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\r\n",
      "Building wheels for collected packages: rouge_score\r\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=9a8709b9a1235b670e0bdd14f59e35d192721d72d2599e318d29f96ee1c523c8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\r\n",
      "Successfully built rouge_score\r\n",
      "Installing collected packages: rouge_score\r\n",
      "Successfully installed rouge_score-0.1.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "443122ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T09:12:51.421856Z",
     "iopub.status.busy": "2025-05-24T09:12:51.421026Z",
     "iopub.status.idle": "2025-05-24T09:36:41.242127Z",
     "shell.execute_reply": "2025-05-24T09:36:41.241363Z"
    },
    "papermill": {
     "duration": 1437.131638,
     "end_time": "2025-05-24T09:36:41.243243",
     "exception": false,
     "start_time": "2025-05-24T09:12:44.111605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Evaluating: 100%|██████████| 2000/2000 [23:48<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.1985\n",
      "Average ROUGE-1 F1 Score: 0.6563\n",
      "Average ROUGE-2 F1 Score: 0.4046\n",
      "Average ROUGE-L F1 Score: 0.5577\n",
      "Number of valid samples: 2000\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "nltk.download('punkt')  # Needed for tokenization in BLEU\n",
    "\n",
    "def extract_target(text, sp):\n",
    "    try:\n",
    "        decoded = sp.decode(text)\n",
    "        if 'Target:' in decoded:\n",
    "            target = decoded.split('Target:')[1].strip()\n",
    "            if '[EOS]' in target:\n",
    "                target = target.split('[EOS]')[0].strip()\n",
    "            return target\n",
    "        print(f\"Warning: No 'Target:' in decoded text: {decoded}\")\n",
    "        return ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding text: {e}, text: {text}\")\n",
    "        return ''\n",
    "\n",
    "def prepare_input(source_text, sp):\n",
    "    \"\"\"Prepare input containing only Source: ... Target:\"\"\"\n",
    "    input_text = f\"Source: {source_text.strip()} Target:\"\n",
    "    input_ids = sp.encode(input_text, out_type=int)\n",
    "    return input_ids\n",
    "\n",
    "model.eval()\n",
    "references = []\n",
    "hypotheses = []\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "max_gen_len = 200\n",
    "max_samples = 2000\n",
    "max_seq_len = 512\n",
    "\n",
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(tqdm(test_loader, desc=\"Evaluating\", total=min(max_samples, len(test_loader)))):\n",
    "        if batch_idx >= max_samples:\n",
    "            break\n",
    "\n",
    "        batch = batch.to(device)\n",
    "        if torch.any(batch < 0) or torch.any(batch >= 52000):\n",
    "            print(f\"Warning: Invalid token in batch {batch_idx}\")\n",
    "            continue\n",
    "\n",
    "        # Extract Source and Target from batch\n",
    "        full_text = sp.decode(batch[0].tolist())\n",
    "        if 'Source:' not in full_text or 'Target:' not in full_text:\n",
    "            print(f\"Batch {batch_idx}: Invalid input format: {full_text}\")\n",
    "            continue\n",
    "        source_text = full_text.split('Target:')[0].replace('Source:', '').strip()\n",
    "        ref = full_text.split('Target:')[1].split('[EOS]')[0].strip()\n",
    "\n",
    "        # Prepare input containing only Source\n",
    "        input_ids = prepare_input(source_text, sp)\n",
    "        generated = input_ids.copy()\n",
    "\n",
    "        for _ in range(max_gen_len):\n",
    "            if len(generated) > max_seq_len:\n",
    "                generated = generated[:max_seq_len]\n",
    "                print(f\"Warning: Truncated sequence to {max_seq_len} tokens in batch {batch_idx}\")\n",
    "                break\n",
    "\n",
    "            input_tensor = torch.tensor([generated], dtype=torch.long).to(device)\n",
    "            output = model(input_tensor)\n",
    "            next_token = output[:, -1, :].argmax(dim=-1).item()\n",
    "            if next_token < 0 or next_token >= 52000:\n",
    "                print(f\"Warning: Invalid token generated in batch {batch_idx}: {next_token}\")\n",
    "                break\n",
    "            generated.append(next_token)\n",
    "            if next_token == sp.eos_id():\n",
    "                break\n",
    "\n",
    "        hyp = extract_target(generated, sp)\n",
    "\n",
    "        if not hyp or not ref:\n",
    "            print(f\"Batch {batch_idx}: Empty hyp or ref: hyp='{hyp}', ref='{ref}'\")\n",
    "            continue\n",
    "\n",
    "        # Tokenize for BLEU\n",
    "        hyp_tokens = nltk.word_tokenize(hyp.lower())\n",
    "        ref_tokens = nltk.word_tokenize(ref.lower())\n",
    "        references.append([ref_tokens])\n",
    "        hypotheses.append(hyp_tokens)\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        try:\n",
    "            scores = scorer.score(ref, hyp)\n",
    "            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating ROUGE score for batch {batch_idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "# Calculate BLEU and ROUGE\n",
    "if references and hypotheses:\n",
    "    bleu = corpus_bleu(references, hypotheses)\n",
    "    avg_rouge1 = sum(rouge_scores['rouge1']) / len(rouge_scores['rouge1']) if rouge_scores['rouge1'] else 0\n",
    "    avg_rouge2 = sum(rouge_scores['rouge2']) / len(rouge_scores['rouge2']) if rouge_scores['rouge2'] else 0\n",
    "    avg_rougeL = sum(rouge_scores['rougeL']) / len(rouge_scores['rougeL']) if rouge_scores['rougeL'] else 0\n",
    "    print(f'BLEU Score: {bleu:.4f}')\n",
    "    print(f'Average ROUGE-1 F1 Score: {avg_rouge1:.4f}')\n",
    "    print(f'Average ROUGE-2 F1 Score: {avg_rouge2:.4f}')\n",
    "    print(f'Average ROUGE-L F1 Score: {avg_rougeL:.4f}')\n",
    "    print(f'Number of valid samples: {len(references)}')\n",
    "else:\n",
    "    print(\"Error: No valid references or hypotheses for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bee96270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T09:36:55.603218Z",
     "iopub.status.busy": "2025-05-24T09:36:55.602524Z",
     "iopub.status.idle": "2025-05-24T09:36:56.046801Z",
     "shell.execute_reply": "2025-05-24T09:36:56.046094Z"
    },
    "papermill": {
     "duration": 7.679226,
     "end_time": "2025-05-24T09:36:56.047931",
     "exception": false,
     "start_time": "2025-05-24T09:36:48.368705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /kaggle/working/best_model.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (embedding): Embedding(52000, 256)\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x DecoderBlock(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc_out): Linear(in_features=256, out_features=52000, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Khởi tạo thiết bị\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = GPTModel(vocab_size=52000).to(device)\n",
    "\n",
    "model_path = '/kaggle/working/best_model.pt'\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True, map_location=torch.device('cpu')))\n",
    "    print(\"Model loaded successfully from /kaggle/working/best_model.pt\")\n",
    "else:\n",
    "    print(\"Model file not found.\")\n",
    "\n",
    "\n",
    "# Đặt mô hình ở chế độ evaluation (cho inference)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a82c8091",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T09:37:10.512639Z",
     "iopub.status.busy": "2025-05-24T09:37:10.511958Z",
     "iopub.status.idle": "2025-05-24T09:37:14.029580Z",
     "shell.execute_reply": "2025-05-24T09:37:14.028867Z"
    },
    "papermill": {
     "duration": 10.824747,
     "end_time": "2025-05-24T09:37:14.030791",
     "exception": false,
     "start_time": "2025-05-24T09:37:03.206044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Although it was raining, we decided to go for a walk in the park.\n",
      "Translation: mặc dù nó đang mưa chúng tôi quyết định đi dạo trong công viên\n",
      "\n",
      "Input: She usually studies at the library because it's quieter than her house.\n",
      "Translation: cô ấy thường nghiên cứu tại thư viện bởi vì nó không còn là nhà của cô ấy\n",
      "\n",
      "Input: If you don’t eat breakfast, you might feel tired before lunchtime.\n",
      "Translation: nếu bạn không ăn bữa sáng bạn có thể cảm thấy mệt mỏi trước khi con quỷ\n",
      "\n",
      "Input: I have been learning English for two years, and I really enjoy it.\n",
      "Translation: tôi đã học tiếng anh trong hai năm và tôi thực sự thích nó\n",
      "\n",
      "Input: He didn’t go to the party because he was feeling a bit sick.\n",
      "Translation: ông ấy không đi tiệc vì anh ấy cảm thấy ốm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tải tokenizer\n",
    "sp = spm.SentencePieceProcessor(model_file='/kaggle/working/ted2020_spm.model')\n",
    "\n",
    "# Hàm dịch\n",
    "def translate(text: str, model, sp, device, max_len=200):\n",
    "    model.eval()\n",
    "    input_text = f\"Source: {clean_text(text)} Target:\"\n",
    "    input_ids = sp.encode(input_text, out_type=int)\n",
    "    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    generated = input_ids\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(torch.tensor([generated], dtype=torch.long).to(device))\n",
    "            next_token = output[:, -1, :].argmax(dim=-1).item()\n",
    "            generated.append(next_token)\n",
    "            if next_token == sp.eos_id():\n",
    "                break\n",
    "    \n",
    "    decoded = sp.decode(generated)\n",
    "    if 'Target:' in decoded:\n",
    "        return decoded.split('Target:')[1].split('[EOS]')[0].strip()\n",
    "    return ''\n",
    "\n",
    "# Hàm clean_text\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Ví dụ\n",
    "texts = [\n",
    "    \"Although it was raining, we decided to go for a walk in the park.\",\n",
    "    \"She usually studies at the library because it's quieter than her house.\",\n",
    "    \"If you don’t eat breakfast, you might feel tired before lunchtime.\",\n",
    "    \"I have been learning English for two years, and I really enjoy it.\",\n",
    "    \"He didn’t go to the party because he was feeling a bit sick.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    translation = translate(text, model, sp, device)\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Translation: {translation}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7319512,
     "sourceId": 11663112,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7460038,
     "sourceId": 11870903,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24509.972227,
   "end_time": "2025-05-24T09:37:24.520269",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-24T02:48:54.548042",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

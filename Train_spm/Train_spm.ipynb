{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897a8a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_text(text):\n",
    "    # Chuyển thành chữ thường\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Loại bỏ tất cả dấu câu ngoại trừ dấu chấm\n",
    "    text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "    \n",
    "    # Thay thế nhiều khoảng trắng bằng một khoảng trắng duy nhất và loại bỏ khoảng trắng ở đầu/cuối\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Dataset paths\n",
    "en_file_path = 'TED2020.en-vi.en'\n",
    "vi_file_path = 'TED2020.en-vi.vi'\n",
    "\n",
    "# Read and clean data\n",
    "try:\n",
    "    with open(en_file_path, 'r', encoding='utf-8') as en_file, \\\n",
    "         open(vi_file_path, 'r', encoding='utf-8') as vi_file, \\\n",
    "         open('cleaned.en', 'w', encoding='utf-8') as en_out, \\\n",
    "         open('cleaned.vi', 'w', encoding='utf-8') as vi_out:\n",
    "        for en_line, vi_line in zip(en_file, vi_file):\n",
    "            en_clean = clean_text(en_line)\n",
    "            vi_clean = clean_text(vi_line)\n",
    "            if en_clean and vi_clean and len(en_clean.split()) > 1 and len(vi_clean.split()) > 1:\n",
    "                en_out.write(en_clean + '\\n')\n",
    "                vi_out.write(vi_clean + '\\n')\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: File not found. Check the file paths: {e}\")\n",
    "    exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data cleaning: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Combine data for GPT (English -> Vietnamese)\n",
    "try:\n",
    "    with open('cleaned.en', 'r', encoding='utf-8') as en_file, \\\n",
    "         open('cleaned.vi', 'r', encoding='utf-8') as vi_file, \\\n",
    "         open('gpt_data.txt', 'w', encoding='utf-8') as out_file:\n",
    "        for en_line, vi_line in zip(en_file, vi_file):\n",
    "            combined = f\"Source: {en_line.strip()} Target: {vi_line.strip()} [EOS]\\n\"\n",
    "            out_file.write(combined)\n",
    "except Exception as e:\n",
    "    print(f\"Error combining data: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "224fc565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training spm took 20.18 seconds\n",
      "Loading tokenizer took 0.04 seconds\n",
      "Sample tokens from BPE vocabulary:\n",
      "Token 0: <pad>\n",
      "Token 1: <s>\n",
      "Token 2: </s>\n",
      "Token 3: <unk>\n",
      "Token 4: [EOS]\n",
      "Token 5: [PAD]\n",
      "Token 6: [BOS]\n",
      "Token 7: Source:\n",
      "Token 8: Target:\n",
      "Token 9: ▁t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing data with BPE: 100%|██████████| 316928/316928 [00:27<00:00, 11417.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data took 28.13 seconds\n",
      "Pad ID: 0\n",
      "BOS ID: 1\n",
      "EOS ID: 2\n",
      "UNK ID: 3\n",
      "Vocab size: 52000\n",
      "Example sentence: Source: i like to learn new things Target: tôi thích học những thứ mới [EOS]\n",
      "BPE tokenized: ['▁', 'Source:', '▁i', '▁like', '▁to', '▁learn', '▁new', '▁things', '▁', 'Target:', '▁tôi', '▁thích', '▁học', '▁những', '▁thứ', '▁mới', '▁', '[EOS]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Train SentencePiece with BPE\n",
    "\"\"\"\n",
    "BPE (Byte Pair Encoding) là một thuật toán nén và token hóa văn bản. \n",
    "Cách hoạt động:\n",
    "1. Bắt đầu với tất cả các ký tự riêng lẻ trong văn bản (bao gồm cả ký hiệu đặc biệt).\n",
    "2. Lặp lại: Tìm cặp ký tự (hoặc subword) xuất hiện thường xuyên nhất và hợp nhất chúng thành một token mới.\n",
    "3. Tiếp tục hợp nhất cho đến khi đạt được kích thước từ vựng mong muốn (vocab_size).\n",
    "4. Kết quả là một từ vựng chứa các subword, giúp xử lý các từ hiếm hoặc từ mới hiệu quả.\n",
    "SentencePiece áp dụng BPE để tạo ra các token subword, phù hợp với các tác vụ dịch máy.\n",
    "\"\"\"\n",
    "start_time = time.time()\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='gpt_data.txt',\n",
    "    model_prefix='ted2020_spm',\n",
    "    vocab_size=52000,\n",
    "    character_coverage=1.0,\n",
    "    model_type='bpe',\n",
    "    user_defined_symbols=['[EOS]', '[PAD]', '[BOS]', 'Source:', 'Target:'],\n",
    "    pad_id=0,\n",
    "    bos_id=1,\n",
    "    eos_id=2,\n",
    "    unk_id=3\n",
    ")\n",
    "print(f\"Training spm took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "sp = spm.SentencePieceProcessor(model_file='ted2020_spm.model')\n",
    "print(f\"Loading tokenizer took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Hiển thị một số token trong từ vựng để minh họa BPE\n",
    "print(\"Sample tokens from BPE vocabulary:\")\n",
    "with open('ted2020_spm.vocab', 'r', encoding='utf-8') as vocab_file:\n",
    "    for i, line in enumerate(vocab_file):\n",
    "        if i >= 10:  # In 10 token đầu tiên\n",
    "            break\n",
    "        token, _ = line.strip().split('\\t')\n",
    "        print(f\"Token {i}: {token}\")\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_gpt_file(input_file, output_file, sp):\n",
    "    \"\"\"\n",
    "    Hàm này mã hóa văn bản thành các token số nguyên sử dụng BPE.\n",
    "    Mỗi dòng văn bản được chia thành các subword dựa trên từ vựng BPE đã huấn luyện.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f_in:\n",
    "            lines = f_in.readlines()\n",
    "        with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "            for line in tqdm(lines, desc=\"Tokenizing data with BPE\"):\n",
    "                tokens = sp.encode(line.strip(), out_type=int)\n",
    "                valid_tokens = [t for t in tokens if 0 <= t < 32000]\n",
    "                if valid_tokens:\n",
    "                    f_out.write(' '.join(map(str, valid_tokens)) + '\\n')\n",
    "    except Exception as e:\n",
    "        print(f\"Error tokenizing data: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "start_time = time.time()\n",
    "tokenize_gpt_file('gpt_data.txt', 'tokenized_gpt.txt', sp)\n",
    "print(f\"Tokenizing data took {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Verify special tokens\n",
    "print(f\"Pad ID: {sp.pad_id()}\")\n",
    "print(f\"BOS ID: {sp.bos_id()}\")\n",
    "print(f\"EOS ID: {sp.eos_id()}\")\n",
    "print(f\"UNK ID: {sp.unk_id()}\")\n",
    "print(f\"Vocab size: {sp.get_piece_size()}\")\n",
    "\n",
    "# Minh họa cách BPE mã hóa một câu ví dụ\n",
    "example_sentence = \"Source: i like to learn new things Target: tôi thích học những thứ mới [EOS]\"\n",
    "encoded = sp.encode(example_sentence, out_type=str)  # out_type=str để thấy các subword\n",
    "print(f\"Example sentence: {example_sentence}\")\n",
    "print(f\"BPE tokenized: {encoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
